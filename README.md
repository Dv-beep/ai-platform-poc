# AI Knowledge Platform â€“ RAG Proof of Concept

This repository contains a self-hosted AI Knowledge Platform built around **Retrieval-Augmented Generation (RAG)**.

The goal of this project is to provide a secure, internal-only AI assistant that can answer questions using an organizationâ€™s own knowledge base (SOPs, policies, internal docs) without sending data to external APIs.

---

## ğŸ” High-Level Overview

**Core idea:**  
Upload or mount internal documents â†’ index them into a vector database â†’ query them through a RAG API â†’ interact via a chat UI.

**Tech stack (example â€“ adjust as needed):**

- **LLM Runtime:** Ollama (local models, e.g. `llama3`, `phi-4`, etc.)
- **Chat UI:** Open WebUI
- **Vector DB:** ChromaDB
- **RAG API:** FastAPI service that:
  - accepts a user query  
  - retrieves relevant docs from ChromaDB  
  - builds a grounded prompt  
  - calls Ollama and returns an answer + sources
- **Indexer Service:** Python service that:
  - reads documents from a mounted KB directory  
  - chunks, embeds, and writes to ChromaDB
- **Reverse Proxy / TLS:** Caddy (or Nginx)
- **Orchestration:** Docker Compose

---

## ğŸ§± Architecture

### Components

1. **Open WebUI**
   - Frontend chat experience for users
   - Sends RAG requests to the `rag-api` service

2. **RAG API (`services/rag-api`)**
   - REST API for RAG queries
   - Endpoints like:
     - `POST /api/rag/query` â€“ ask a question with optional filters
     - `GET /api/health` â€“ health check
   - Orchestrates:
     - search in ChromaDB
     - building the final prompt
     - calling Ollama
     - formatting the response + citations

3. **ChromaDB**
   - Stores vector embeddings for KB documents
   - Collections organized by namespace (e.g. `kb_sops`, `policies`, `howto`)

4. **KB Indexer (`services/kb-indexer`)**
   - Ingests files from a directory (e.g. `kb-samples/`)
   - Normalizes and chunks content (PDF, DOCX, MD, etc.)
   - Uses a sentence-transformer / embedding model
   - Pushes embeddings + metadata into ChromaDB

5. **Ollama**
   - Runs local LLMs
   - Keeps all prompts and context on-prem

6. **Caddy / Reverse Proxy**
   - Terminates TLS (if configured)
   - Routes traffic to:
     - `/` â†’ Open WebUI
     - `/api/rag/` â†’ RAG API

---

## ğŸ—‚ï¸ Repository Structure

```text
.
â”œâ”€â”€ README.md
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture-overview.md
â”‚   â”œâ”€â”€ rag-sequence-diagram.md
â”‚   â”œâ”€â”€ screenshots/
â”‚   â”‚   â”œâ”€â”€ openwebui-home.png
â”‚   â”‚   â””â”€â”€ rag-chat-example.png
â”œâ”€â”€ deploy/
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â”œâ”€â”€ .env.example
â”‚   â”œâ”€â”€ caddy/
â”‚   â”‚   â””â”€â”€ Caddyfile
â”‚   â”œâ”€â”€ openwebui/
â”‚   â”‚   â””â”€â”€ openwebui.yaml
â”‚   â””â”€â”€ chromadb/
â”‚       â””â”€â”€ chromadb.config.yaml
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ rag-api/
â”‚   â””â”€â”€ kb-indexer/
â”œâ”€â”€ kb-samples/
â””â”€â”€ scripts/
```
See docs/architecture-overview.md for diagrams and more detail.

---

ğŸš€ Getting Started

1. Prerequisites
	â€¢	Docker & Docker Compose
	â€¢	(Optional) NVIDIA GPU drivers + CUDA for GPU acceleration with Ollama

2. Clone the repo

git clone https://github.com/<your-username>/ai-platform-poc.git
cd ai-platform-poc

3. Configure environment

Copy the example env file and adjust values:
cp deploy/.env.example deploy/.env   

Configure:
	â€¢	OLLAMA_BASE_URL
	â€¢	CHROMA_HOST / CHROMA_PORT
	â€¢	RAG_API_PORT
	â€¢	OPENWEBUI_PORT
	â€¢	Any auth/API keys if you add them later






